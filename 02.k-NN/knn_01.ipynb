{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "k-Nearest-Neighbors (kNN) is a simple technique for classification. The idea behind\n",
    "it is that similar data points should have the same class, at least most of the time.\n",
    "This method is very intuitive and has proven itself in many domains including\n",
    "recommendation systems, anomaly detection, image/text classification and more.\n",
    "\n",
    "In what follows we present a detailed example of a multi-class classification objective. The dataset we use contains information collected by the US Geological Survey and the US Forest Service about wilderness areas in northern Colorado. The features are measurements like soil type, elevation, and distance to water, and the labels encode the type of trees - the forest cover type - for each location. The machine learning task is to predict the cover type in a given location using the features. Overall there are seven cover types.\n",
    "\n",
    "The notebook has two sections. In the first, we use Amazon SageMaker's python SDK in order to train a kNN classifier in its simplest setting. We explain the components common to all Amazon SageMaker's algorithms including uploading data to Amazon S3, training a model, and setting up an endpoint for online inference. In the second section we dive deeper into the details of Amazon SageMaker kNN. We explain the different knobs (hyper-parameters) associated with it, and demonstrate how each setting can lead to a somewhat different accuracy and latency at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We're about to work with the UCI Machine Learning Repository Covertype  dataset  ([covtype](https://archive.ics.uci.edu/ml/datasets/covertype)) (copyright Jock A. Blackard and Colorado State University). It's a labeled dataset where each entry describes a geographic area, and the label is a type of forest cover. There are 7 possible labels and we aim to solve the mult-class classification problem using kNN.\n",
    "We begin by downloading the dataset and moving it to a temporary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-28 05:02:58--  https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11240707 (11M) [application/x-httpd-php]\n",
      "Saving to: ‘covtype.data.gz’\n",
      "\n",
      "covtype.data.gz     100%[===================>]  10.72M  13.6MB/s    in 0.8s    \n",
      "\n",
      "2020-04-28 05:02:59 (13.6 MB/s) - ‘covtype.data.gz’ saved [11240707/11240707]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/kNN/01/raw\n",
    "!mkdir /root/kNN/01/raw\n",
    "!mv /root/kNN/01/covtype.data.gz /root/kNN/01/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The forest cover type is the classification problem. The order of this listing corresponds to the order of numerals along the rows of the database.\n",
    "\n",
    "Name / Data Type / Measurement / Description\n",
    "\n",
    "- Elevation / quantitative /meters / Elevation in meters\n",
    "- Aspect / quantitative / azimuth / Aspect in degrees azimuth\n",
    "- Slope / quantitative / degrees / Slope in degrees\n",
    "- Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features\n",
    "- Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features\n",
    "- Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway\n",
    "- Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice\n",
    "- Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice\n",
    "- Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice\n",
    "- Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points\n",
    "- Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\n",
    "- Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation\n",
    "- Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = \"/root/kNN/01/\"\n",
    "\n",
    "data_file_name = \"raw/covtype.data.gz\"\n",
    "processed_subdir = \"standardized\"\n",
    "\n",
    "raw_data_file = os.path.join(cwd, data_file_name)\n",
    "\n",
    "# Taining Data\n",
    "train_features_file = os.path.join(cwd, processed_subdir, \"train/csv/features.csv\")\n",
    "train_labels_file = os.path.join(cwd, processed_subdir, \"train/csv/labels.csv\")\n",
    "\n",
    "# Test Data \n",
    "test_features_file = os.path.join(cwd, processed_subdir, \"test/csv/features.csv\")\n",
    "test_labels_file = os.path.join(cwd, processed_subdir, \"test/csv/labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/kNN/01/raw/covtype.data.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Raw Data from /root/kNN/01/raw/covtype.data.gz\n"
     ]
    }
   ],
   "source": [
    "# Read raw data \n",
    "print(f\"Reading Raw Data from {raw_data_file}\")\n",
    "raw = np.loadtxt(raw_data_file, delimiter=',')                   # np.loadtxt - If the filename extension is .gz or .bz2, \\\n",
    "                                                                 # the file is first decompressed. Note that generators should \\\n",
    "                                                                 # return byte strings for Python 3k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 55)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.596e+03, 5.100e+01, 3.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        5.000e+00],\n",
       "       [2.590e+03, 5.600e+01, 2.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        5.000e+00],\n",
       "       [2.804e+03, 1.390e+02, 9.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        2.000e+00],\n",
       "       ...,\n",
       "       [2.386e+03, 1.590e+02, 1.700e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        3.000e+00],\n",
       "       [2.384e+03, 1.700e+02, 1.500e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        3.000e+00],\n",
       "       [2.383e+03, 1.650e+02, 1.300e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        3.000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test with a 80/20 split\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(raw)\n",
    "\n",
    "train_size = int(0.8 * raw.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464809"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = raw[:train_size, :-1]\n",
    "train_labels = raw[:train_size, -1]\n",
    "test_features = raw[train_size:, :-1]\n",
    "test_labels = raw[train_size:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464809, 54)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464809,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " array([169467, 226581,  28677,   2198,   7575,  13924,  16387]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels, return_counts=True)                            # We can check class label distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets start with Train data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape = (464809, 54)\n",
      "train_labels shape = (464809,)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "print(f\"train_features shape = {train_features.shape}\")\n",
    "print(f\"train_labels shape = {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buff, train_features, train_labels)\n",
    "buff.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://ml-demo-bucket-suman/knn-01/train/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "import boto3 \n",
    "import os \n",
    "import sagemaker \n",
    "\n",
    "bucket = \"ml-demo-bucket-suman\"\n",
    "prefix = \"knn-01\"\n",
    "key = \"recordio-pb-data\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buff)         # Upload a file-like object to S3.\n",
    "                                                                                          # The file-like object must be in binary mode.\n",
    "                                                                                          # This is a managed transfer which will perform a multipart \n",
    "                                                                                          # upload in multiple threads if necessary\n",
    "\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shall do the same for the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_features shape =  (116203, 54)\n",
      "test_labels shape =  (116203,)\n",
      "Uploaded test data location: s3://ml-demo-bucket-suman/knn-01/test/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "print('test_features shape = ', test_features.shape)\n",
    "print('test_labels shape = ', test_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buff, test_features, test_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\n",
    "s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "print('Uploaded test data location: {}'.format(s3_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We take a moment to explain at a high level, how Machine Learning training and prediction works in Amazon SageMaker. First, we need to train a model. This is a process that given a labeled dataset and hyper-parameters guiding the training process,  outputs a model. Once the training is done, we set up what is called an **endpoint**. An endpoint is a web service that given a request containing an unlabeled data point, or mini-batch of data points, returns a prediction(s).\n",
    "\n",
    "In Amazon SageMaker the training is done via an object called an **estimator**. When setting up the estimator we specify the location (in Amazon S3) of the training data, the path (again in Amazon S3) to the output directory where the model will be serialized, generic hyper-parameters such as the machine type to use during the training process, and kNN-specific hyper-parameters such as the index type, etc. Once the estimator is initialized, we can call its **fit** method in order to do the actual training.\n",
    "\n",
    "Now that we are ready for training, we start with a convenience function that starts a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'404615174143.dkr.ecr.us-east-2.amazonaws.com/knn:1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image_uri(boto3.Session().region_name, \"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "                                        get_execution_role(),\n",
    "                                        train_instance_count=1,\n",
    "                                        train_instance_type='ml.m5.2xlarge',\n",
    "                                        output_path=output_path,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "                                        \n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    if s3_test_data is not None:\n",
    "        fit_input['test'] = s3_test_data\n",
    "    knn.fit(fit_input)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ml-demo-bucket-suman/knn-01/train/recordio-pb-data'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ml-demo-bucket-suman/knn-01/test/recordio-pb-data'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the actual training job. For now, we stick to default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'feature_dim': 54,\n",
    "    'k': 11,\n",
    "    'sample_size': 200000,\n",
    "    'predictor_type': 'classifier', \n",
    "    'index_metric': 'L2'\n",
    "}\n",
    "\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/default_example/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_dim': 54,\n",
       " 'k': 11,\n",
       " 'sample_size': 200000,\n",
       " 'predictor_type': 'classifier',\n",
       " 'index_metric': 'L2'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-28 06:50:59 Starting - Starting the training job...\n",
      "2020-04-28 06:51:01 Starting - Launching requested ML instances...\n",
      "2020-04-28 06:52:00 Starting - Preparing the instances for training......\n",
      "2020-04-28 06:52:43 Downloading - Downloading input data\n",
      "2020-04-28 06:52:43 Training - Downloading the training image...\n",
      "2020-04-28 06:53:30 Uploading - Uploading generated training model\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'index_metric': u'L2', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'faiss_index_ivf_nlists': u'auto', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'sample_size': u'200000', u'feature_dim': u'54', u'index_metric': u'L2', u'predictor_type': u'classifier', u'k': u'11'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Final configuration: {u'index_metric': u'L2', u'predictor_type': u'classifier', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'feature_dim': u'54', u'faiss_index_ivf_nlists': u'auto', u'sample_size': u'200000', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'k': u'11'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 WARNING 139921556141888] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/97f75de0-f6c3-4ac5-bcd7-17bf461e057c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-04-28-06-50-59-690', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-227-43.us-east-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/5c798f7b-046e-4499-a62a-7588f507b6e1', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:507922848584:training-job/knn-2020-04-28-06-50-59-690', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/97f75de0-f6c3-4ac5-bcd7-17bf461e057c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.227.43', 'AWS_REGION': 'us-east-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-04-28-06-50-59-690', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-227-43.us-east-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/5c798f7b-046e-4499-a62a-7588f507b6e1', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:507922848584:training-job/knn-2020-04-28-06-50-59-690', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/97f75de0-f6c3-4ac5-bcd7-17bf461e057c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-east-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-04-28-06-50-59-690', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-227-43.us-east-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/5c798f7b-046e-4499-a62a-7588f507b6e1', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:507922848584:training-job/knn-2020-04-28-06-50-59-690', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/97f75de0-f6c3-4ac5-bcd7-17bf461e057c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.227.43', 'AWS_REGION': 'us-east-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-04-28-06-50-59-690', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-227-43.us-east-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/5c798f7b-046e-4499-a62a-7588f507b6e1', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:507922848584:training-job/knn-2020-04-28-06-50-59-690', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/97f75de0-f6c3-4ac5-bcd7-17bf461e057c', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '1', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.227.43', 'AWS_REGION': 'us-east-2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-04-28-06-50-59-690', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '4', 'HOSTNAME': 'ip-10-0-227-43.us-east-2.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/5c798f7b-046e-4499-a62a-7588f507b6e1', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-2:507922848584:training-job/knn-2020-04-28-06-50-59-690', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 66 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 76 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-04-28 06:53:27.394] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2020-04-28 06:53:27.395] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 0, \"num_examples\": 0, \"num_bytes\": 0}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] nvidia-smi took: 0.025160074234 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 ERROR 139921556141888] nvidia-smi: failed to run (127): /bin/sh: nvidia-smi: command not found\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:27 INFO 139921556141888] Using per-worker sample size = 200000 (Available virtual memory = 30743044096 bytes, GPU free memory = 0 bytes, number of workers = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, and/or decrease mini_batch_size.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1588056808.310069, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1588056808.310018}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-04-28 06:53:28.312] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 937, \"num_examples\": 1, \"num_bytes\": 2420000}\u001b[0m\n",
      "\u001b[34m[2020-04-28 06:53:29.390] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 1078, \"num_examples\": 93, \"num_bytes\": 224967556}\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] push reservoir to kv... 1 num_workers 0 rank\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] ...done (200000)\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 93, \"sum\": 93.0, \"min\": 93}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 93, \"sum\": 93.0, \"min\": 93}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 464809, \"sum\": 464809.0, \"min\": 464809}, \"Total Batches Seen\": {\"count\": 1, \"max\": 93, \"sum\": 93.0, \"min\": 93}, \"Total Records Seen\": {\"count\": 1, \"max\": 464809, \"sum\": 464809.0, \"min\": 464809}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 464809, \"sum\": 464809.0, \"min\": 464809}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1588056809.484854, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\", \"epoch\": 0}, \"StartTime\": 1588056808.312439}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] #throughput_metric: host=algo-1, train throughput=396409.661682 records/second\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] pulled row count... worker 0 rows 200000\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] pulled... worker 0 data (200000, 54) labels (200000, 1) nans 0\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] calling index.train...\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] ...done calling index.train\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] calling index.add...\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] ...done calling index.add\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"model.serialize.time\": {\"count\": 1, \"max\": 24.406909942626953, \"sum\": 24.406909942626953, \"min\": 24.406909942626953}, \"finalize.time\": {\"count\": 1, \"max\": 231.5518856048584, \"sum\": 231.5518856048584, \"min\": 231.5518856048584}, \"initialize.time\": {\"count\": 1, \"max\": 914.4730567932129, \"sum\": 914.4730567932129, \"min\": 914.4730567932129}, \"update.time\": {\"count\": 1, \"max\": 1172.1880435943604, \"sum\": 1172.1880435943604, \"min\": 1172.1880435943604}}, \"EndTime\": 1588056809.741155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1588056807.374551}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/28/2020 06:53:29 INFO 139921556141888] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2628.0150413513184, \"sum\": 2628.0150413513184, \"min\": 2628.0150413513184}, \"setuptime\": {\"count\": 1, \"max\": 19.12093162536621, \"sum\": 19.12093162536621, \"min\": 19.12093162536621}}, \"EndTime\": 1588056809.757454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1588056809.741241}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-28 06:53:37 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n"
     ]
    }
   ],
   "source": [
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=s3_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we mentioned a test set in the training job. When a test set is provided the training job doesn't just produce a model but also applies it to the test set and reports the accuracy. In the logs you can view the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the endpoint\n",
    "\n",
    "Now that we have a trained model, we are ready to run inference. The **knn_estimator** object above contains all the information we need for hosting the model. Below we provide a convenience function that given an estimator, sets up and endpoint that hosts the model. Other than the estimator object, we provide it with a name (string) for the estimator, and an **instance_type**. The **instance_type** is the machine type that will host the model. It is not restricted in any way by the parameter settings of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name)\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the endpoint..\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have our predictor, let's use it on our test dataset. The following code runs on the test dataset, computes the accuracy and the average latency. It splits up the data into 100 batches, each of size roughly 500. Then, each batch is given to the inference service to obtain predictions. Once we have all predictions, we compute their accuracy given the true labels of the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'predicted_label': 2.0}]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data split into 100 batches, of size 1163.\n",
      "time required for predicting 116203 data point: 82.65 seconds\n",
      "accuracy of model: 91.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = np.array_split(test_features, 100)\n",
    "print('data split into 100 batches, of size %d.' % batches[0].shape[0])\n",
    "\n",
    "# obtain an np array with the predictions for the entire test set\n",
    "start_time = time.time()\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    result = predictor.predict(batch)\n",
    "    cur_predictions = np.array([result['predictions'][i]['predicted_label'] for i in range(len(result['predictions']))])\n",
    "    predictions.append(cur_predictions)\n",
    "predictions = np.concatenate(predictions)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "test_size = test_labels.shape[0]\n",
    "num_correct = sum(predictions == test_labels)\n",
    "accuracy = num_correct / float(test_size)\n",
    "print('time required for predicting %d data point: %.2f seconds' % (test_size, run_time))\n",
    "print('accuracy of model: %.1f%%' % (accuracy * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116203"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116203"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 70642,   3188],\n",
       "        [  3612,  38761]],\n",
       "\n",
       "       [[ 55005,   4478],\n",
       "        [  3368,  53352]],\n",
       "\n",
       "       [[108328,    798],\n",
       "        [   645,   6432]],\n",
       "\n",
       "       [[115606,     48],\n",
       "        [   190,    359]],\n",
       "\n",
       "       [[114018,    267],\n",
       "        [   511,   1407]],\n",
       "\n",
       "       [[112317,    443],\n",
       "        [   733,   2710]],\n",
       "\n",
       "       [[111843,    237],\n",
       "        [   400,   3723]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 70642,   3188],\n",
       "        [  3612,  38761]],\n",
       "\n",
       "       [[ 55005,   4478],\n",
       "        [  3368,  53352]],\n",
       "\n",
       "       [[108328,    798],\n",
       "        [   645,   6432]],\n",
       "\n",
       "       [[115606,     48],\n",
       "        [   190,    359]],\n",
       "\n",
       "       [[114018,    267],\n",
       "        [   511,   1407]],\n",
       "\n",
       "       [[112317,    443],\n",
       "        [   733,   2710]],\n",
       "\n",
       "       [[111843,    237],\n",
       "        [   400,   3723]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.92      0.91      0.92     42373\n",
      "         2.0       0.92      0.94      0.93     56720\n",
      "         3.0       0.89      0.91      0.90      7077\n",
      "         4.0       0.88      0.65      0.75       549\n",
      "         5.0       0.84      0.73      0.78      1918\n",
      "         6.0       0.86      0.79      0.82      3443\n",
      "         7.0       0.94      0.90      0.92      4123\n",
      "\n",
      "    accuracy                           0.92    116203\n",
      "   macro avg       0.89      0.83      0.86    116203\n",
      "weighted avg       0.92      0.92      0.92    116203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the endpoint\n",
    "\n",
    "We're now done with the example except a final clean-up act. By setting up the endpoint we started a machine in the cloud and as long as it's not deleted the machine is still up and we are paying for it. Once the endpoint is no longer necessary, we delete it. The following code does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(predictor):\n",
    "    try:\n",
    "        boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "        print('Deleted {}'.format(predictor.endpoint))\n",
    "    except:\n",
    "        print('Already deleted: {}'.format(predictor.endpoint))\n",
    "\n",
    "#delete_endpoint(predictor)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've seen how to both train and host an inference endpoint for kNN. With absolutely zero tuning we obtain an accuracy of 92.2% on the covtype dataset. As a point of reference for grasping the prediction power of the kNN model, a linear model will achieve roughly 72.8% accuracy. There are several advanced issues that we did not discuss. In the next section we will deep-dive into issues such as run-time / latency, and tuning the model while taking into account both the accuracy and its run-time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
